{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32,padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "test_transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=train_transformations, download=True)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=test_transformations, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset, valset = train_test_split(trainset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)\n",
    "#valloader = torch.utils.data.DataLoader(valset, batch_size=4, shuffle=False, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_chan, out_channels=out_chan, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_chan)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_class=10):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.net1 = Net(1,16)\n",
    "        self.net2 = Net(16,32)\n",
    "        self.net3 = Net(32,64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.net4 = Net(64,64)\n",
    "        self.net5 = Net(64,64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.net6 = Net(64,128)\n",
    "        self.net7 = Net(128,128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=4) #change to kernal size 3?\n",
    "        \n",
    "        self.net = nn.Sequential(self.net1,self.net2,self.net3,self.pool1,self.net4,self.net5,self.pool2,self.net6,self.net7,self.pool3,self.avgpool)\n",
    "        self.fc = nn.Linear(in_features=128,out_features=num_class)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.net(input)\n",
    "        output = output.view(-1,128)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(epoch):\n",
    "    lr = 0.001\n",
    "\n",
    "    if epoch > 50:\n",
    "        lr = lr / 1000000\n",
    "    elif epoch > 40:\n",
    "        lr = lr / 100000\n",
    "    elif epoch > 30:\n",
    "        lr = lr / 10000\n",
    "    elif epoch > 20:\n",
    "        lr = lr / 1000\n",
    "    elif epoch > 10:\n",
    "        lr = lr / 100\n",
    "    elif epoch > 5:\n",
    "        lr = lr / 10\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            images, labels = data\n",
    "            # Clear all accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Predict classes using images from the test set\n",
    "            outputs = model(images)\n",
    "            # Compute the loss based on the predictions and actual labels\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "            # Adjust parameters according to the computed gradients\n",
    "            optimizer.step()\n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Print the metrics\n",
    "            if i % 2000 == 1999:\n",
    "                print(\"Epoch {}, Count: {}, Running Loss: {}\".format(epoch+1, i+1,running_loss))\n",
    "        #adjust_learning_rate(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Count: 2000, Running Loss: 5138.777373433113\n",
      "Epoch 1, Count: 4000, Running Loss: 10292.068709373474\n",
      "Epoch 1, Count: 6000, Running Loss: 15472.336253285408\n",
      "Epoch 1, Count: 8000, Running Loss: 20628.961946964264\n",
      "Epoch 1, Count: 10000, Running Loss: 25783.86703646183\n",
      "Epoch 1, Count: 12000, Running Loss: 30919.46484029293\n",
      "Epoch 1, Count: 14000, Running Loss: 36056.45467936993\n",
      "Epoch 2, Count: 2000, Running Loss: 5164.9055869579315\n",
      "Epoch 2, Count: 4000, Running Loss: 10314.213210701942\n",
      "Epoch 2, Count: 6000, Running Loss: 15446.92931163311\n",
      "Epoch 2, Count: 8000, Running Loss: 20582.429592370987\n",
      "Epoch 2, Count: 10000, Running Loss: 25746.230667948723\n",
      "Epoch 2, Count: 12000, Running Loss: 30910.976651906967\n",
      "Epoch 2, Count: 14000, Running Loss: 36081.42679691315\n",
      "Epoch 3, Count: 2000, Running Loss: 5132.892731428146\n",
      "Epoch 3, Count: 4000, Running Loss: 10275.792593121529\n",
      "Epoch 3, Count: 6000, Running Loss: 15437.320172786713\n",
      "Epoch 3, Count: 8000, Running Loss: 20588.90808045864\n",
      "Epoch 3, Count: 10000, Running Loss: 25735.0717086792\n",
      "Epoch 3, Count: 12000, Running Loss: 30919.856851935387\n",
      "Epoch 3, Count: 14000, Running Loss: 36086.42607116699\n",
      "Epoch 4, Count: 2000, Running Loss: 5131.577858090401\n",
      "Epoch 4, Count: 4000, Running Loss: 10281.904252767563\n",
      "Epoch 4, Count: 6000, Running Loss: 15456.312613129616\n",
      "Epoch 4, Count: 8000, Running Loss: 20628.43509697914\n",
      "Epoch 4, Count: 10000, Running Loss: 25775.041080117226\n",
      "Epoch 4, Count: 12000, Running Loss: 30908.222727537155\n",
      "Epoch 4, Count: 14000, Running Loss: 36070.450891017914\n"
     ]
    }
   ],
   "source": [
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './fashion-MNIST_e5.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNet()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        # Predict classes using images from the test set\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        \n",
    "        test_acc += torch.sum(prediction == labels.data)\n",
    "\n",
    "    # Compute the average acc and loss over all 10000 test images\n",
    "    test_acc = test_acc / 10000\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py382] *",
   "language": "python",
   "name": "conda-env-py382-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
